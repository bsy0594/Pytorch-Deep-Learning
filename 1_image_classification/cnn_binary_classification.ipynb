{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 사전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 import\n",
    "import glob\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import pandas as pd \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d5049d9490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 난수 시드 설정\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorch_data(Dataset):\n",
    "    \n",
    "    def __init__(self,data_dir,transform,data_type=\"train\"):      \n",
    "    \n",
    "        # Get Image File Names\n",
    "        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n",
    "        \n",
    "        file_names = os.listdir(cdm_data) # get list of images in that directory  \n",
    "        idx_choose = np.random.choice(np.arange(len(file_names)), \n",
    "                                      3000,\n",
    "                                      replace=False).tolist()\n",
    "        file_names_sample = [file_names[x] for x in idx_choose]\n",
    "        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n",
    "        \n",
    "        # Get Labels\n",
    "        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n",
    "        labels_df=pd.read_csv(labels_data)\n",
    "        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n",
    "        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n",
    "        self.transform = transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames) # size of dataset\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n",
    "        image = self.transform(image) # Apply Specific Transformation to Image\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 46, 46]) tensor(0.0688) tensor(0.9951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((46,46))])\n",
    "\n",
    "# Define an object of the custom dataset for the train folder.\n",
    "data_dir = 'C:/Users/82102/Image_Binary_Classification/histopathologic-cancer-detection/'\n",
    "img_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images\n",
    "\n",
    "# load an example tensor\n",
    "img,label=img_dataset[10]\n",
    "print(img.shape,torch.min(img),torch.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 2400\n",
      "validation dataset size: 600\n"
     ]
    }
   ],
   "source": [
    "len_img=len(img_dataset)\n",
    "len_train=int(0.8*len_img)\n",
    "len_val=len_img-len_train\n",
    "\n",
    "# Split Pytorch tensor\n",
    "train_ts,val_ts=random_split(img_dataset,\n",
    "                             [len_train,len_val]) # random split 80/20\n",
    "\n",
    "print(\"train dataset size:\", len(train_ts))\n",
    "print(\"validation dataset size:\", len(val_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomVerticalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the following transformations for the training dataset\n",
    "tr_transf = transforms.Compose([\n",
    "#     transforms.Resize((40,40)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(45),         \n",
    "#     transforms.RandomResizedCrop(50,scale=(0.8,1.0),ratio=(1.0,1.0)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# For the validation dataset, we don't need any augmentation; simply convert images into tensors\n",
    "val_transf = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# After defining the transformations, overwrite the transform functions of train_ts, val_ts\n",
    "train_ts.transform=tr_transf\n",
    "val_ts.transform=val_transf\n",
    "\n",
    "# The subset can also have transform attribute (if we asign)\n",
    "train_ts.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataloader 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training DataLoader\n",
    "train_dl = DataLoader(train_ts,\n",
    "                      batch_size=32, \n",
    "                      shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dl = DataLoader(val_ts,\n",
    "                    batch_size=32,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 네트워크 모델 작성, 4. 순전파함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Neural Network\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*8*init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self,X):\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X)); \n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv4(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        X = X.view(-1, self.num_flatten)\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Predefined Parameters\n",
    "params_model={\n",
    "        \"shape_in\": (3,46,46), \n",
    "        \"initial_filters\": 8,    \n",
    "        \"num_fc1\": 100,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"num_classes\": 2}\n",
    "\n",
    "# Create instantiation of Network class\n",
    "cnn_model = Network(params_model)\n",
    "\n",
    "# define computation hardware approach (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 손실함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 최적화기법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(cnn_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 학습 및 검증 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Helper Functions'''\n",
    "\n",
    "# Function to get the learning rate\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Function to compute the loss value per batch of data\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    \n",
    "    loss = loss_func(output, target) # get loss\n",
    "    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n",
    "    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
    "def loss_epoch(model,loss_func,dataset_dl,opt=None):\n",
    "    \n",
    "    run_loss=0.0 \n",
    "    t_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "\n",
    "    # internal loop over dataset\n",
    "    for xb, yb in dataset_dl:\n",
    "        # move batch to device\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        output=model(xb) # get model output\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n",
    "        run_loss+=loss_b        # update running loss\n",
    "\n",
    "        if metric_b is not None: # update running metric\n",
    "            t_metric+=metric_b    \n",
    "    \n",
    "    loss=run_loss/float(len_data)  # average loss value\n",
    "    metric=t_metric/float(len_data) # average metric value\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train={\n",
    " \"train\": train_dl,\"val\": val_dl,\n",
    " \"epochs\": 50,\n",
    " \"optimiser\": optim.Adam(cnn_model.parameters(),\n",
    "                         lr=3e-4),\n",
    " \"lr_change\": ReduceLROnPlateau(opt,\n",
    "                                mode='min',\n",
    "                                factor=0.5,\n",
    "                                patience=20,\n",
    "                                verbose=0),\n",
    " \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n",
    " \"weight_path\": \"weights.pt\",\n",
    " \"check\": False, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def train_val(model, params,verbose=False):\n",
    "    \n",
    "    # Get the parameters\n",
    "    epochs=params[\"epochs\"]\n",
    "    loss_func=params[\"f_loss\"]\n",
    "    opt=params[\"optimiser\"]\n",
    "    train_dl=params[\"train\"]\n",
    "    val_dl=params[\"val\"]\n",
    "    lr_scheduler=params[\"lr_change\"]\n",
    "    weight_path=params[\"weight_path\"]\n",
    "    \n",
    "    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n",
    "    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n",
    "    best_loss=float('inf') # initialize best loss to a large value\n",
    "    \n",
    "    ''' Train Model n_epochs '''\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        ''' Get the Learning Rate '''\n",
    "        current_lr=get_lr(opt)\n",
    "        if(verbose):\n",
    "            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Train Model Process\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n",
    "\n",
    "        # collect losses\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Evaluate Model Process\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n",
    "        \n",
    "        # store best model\n",
    "        if(val_loss < best_loss):\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), weight_path)\n",
    "            if(verbose):\n",
    "                print(\"Copied best model weights!\")\n",
    "        \n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            if(verbose):\n",
    "                print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n",
    "            print(\"-\"*10) \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba876a382e2340fe84d87f0e25d5cb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_train={\n",
    " \"train\": train_dl,\"val\": val_dl,\n",
    " \"epochs\": 50,\n",
    " \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n",
    " \"lr_change\": ReduceLROnPlateau(opt,\n",
    "                                mode='min',\n",
    "                                factor=0.5,\n",
    "                                patience=20,\n",
    "                                verbose=0),\n",
    " \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n",
    " \"weight_path\": \"weights.pt\",\n",
    "}\n",
    "\n",
    "''' Actual Train / Evaluation of CNN Model '''\n",
    "# train and validate the model\n",
    "\n",
    "cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorchdata_test(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, transform,data_type=\"train\"):\n",
    "        \n",
    "        path2data = os.path.join(data_dir,data_type)\n",
    "        filenames = os.listdir(path2data)\n",
    "        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n",
    "        \n",
    "        # labels are in a csv file named train_labels.csv\n",
    "        csv_filename=\"sample_submission.csv\"\n",
    "        path2csvLabels=os.path.join(data_dir,csv_filename)\n",
    "        labels_df=pd.read_csv(path2csvLabels)\n",
    "        \n",
    "        # set data frame index to id\n",
    "        labels_df.set_index(\"id\", inplace=True)\n",
    "        \n",
    "        # obtain labels from data frame\n",
    "        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n",
    "        self.transform = transform       \n",
    "        \n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.full_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx]) # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load any model weights for the model\n",
    "cnn_model.load_state_dict(torch.load('weights.pt'))\n",
    "\n",
    "# sample submission\n",
    "path_sub = \"C:/Users/82102/Image_Binary_Classification/histopathologic-cancer-detection/sample_submission.csv\"\n",
    "\n",
    "data_dir = \"C:/Users/82102/Image_Binary_Classification/histopathologic-cancer-detection\"\n",
    "\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((46,46))])\n",
    "\n",
    "img_dataset_test = pytorchdata_test(data_dir,data_transformer,data_type=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,dataset,device,num_classes=2):\n",
    "    \n",
    "    len_data=len(dataset)\n",
    "    y_out=torch.zeros(len_data,num_classes) # initialize output tensor on CPU\n",
    "    y_gt=np.zeros((len_data),dtype=\"uint8\") # initialize ground truth on CPU\n",
    "    model=model.to(device) # move model to device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len_data)):\n",
    "            x,y=dataset[i]\n",
    "            y_gt[i]=y\n",
    "            y_out[i]=model(x.unsqueeze(0).to(device))\n",
    "\n",
    "    return y_out.numpy(),y_gt            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1be73c72b41d1b362abccb8b6362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57458,)\n",
      "[0.47708148 0.8111153  0.2311229  0.97962856 0.06620736]\n"
     ]
    }
   ],
   "source": [
    "y_test_out,_ = inference(cnn_model,img_dataset_test, device)     \n",
    "\n",
    "# class predictions 0,1\n",
    "y_test_pred=np.argmax(y_test_out,axis=1)\n",
    "\n",
    "# probabilities of predicted selection\n",
    "# return F.log_softmax(x, dim=1) ie.\n",
    "preds = np.exp(y_test_out[:, 1])\n",
    "\n",
    "print(preds.shape)\n",
    "print(preds[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
